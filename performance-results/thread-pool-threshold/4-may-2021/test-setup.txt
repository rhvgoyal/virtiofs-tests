Purpose
=======
Run tests with and without --thread-pool-threshold=<nr-requests> knob.
This knob allows to send requests to thread pool only if number of requests
received on queue are greater than nr-requests. All the requests which
are less than nr-requests are processed inline.

Methodology
===========
I have basically run bunch of fio jobs to get a sense of speed of
various operations. I wrote a simple wrapper script to run fio jobs
3 times and take their average and report it. These scripts and fio
jobs are available here.

https://github.com/rhvgoyal/virtiofs-tests

I have a fast optane SSD on host where I setup a test directory and exported
that directory inside guest using virtiofs and ran tests inside guests.

I have used caching mode cache=auto for vitiofs. Ran virtiofsd with option
"-o allow_direct_io" so that host cache is bypassed. Ran all tests with
direct I/O so that guest cache is bypassed too. Only ran tests with 
ioengine psync and libaio as these work with direct I/O. I believe ioengine
mmap can't do direct I/O .

Used numactl to bind both virtiofs and qemu to node 0. This helps with
reducing numa affects and reduce run to run performance variation.

Test Setup
-----------
- A fedora 29 host with 376Gi RAM, 2 sockets (20 cores per socket, 2
  threads per core)

- Using optane SSD on host as backing store. 4 fio files of 4G each.

- Created a VM with 32 VCPUS and 64GB memory.

- Host kernel: 5.12.0-rc1
- Guest kernel: 5.12.0

- QEMU emulator version 5.2.94 (v6.0.0-rc4-dirty)

virtiofsd command line
----------------------
- Used following options for virtiofsd.

./build/tools/virtiofsd/virtiofsd --socket-path=/tmp/vhostqemu -o source=/mnt/ssd/virtiofs-source/ -o cache=auto -o xattr --thread-pool-size=16 -o allow_direct_io --daemonize

virtiofs qemu configuration
---------------------------
-object memory-backend-file,id=mem,size=64G,mem-path=/dev/shm,share=on -numa node,memdev=mem"

-chardev socket,id=char0,path=/tmp/vhostqemu -device vhost-user-fs-pci,chardev=char0,tag=myfs,queue-size=1024

Test Results
------------
- Results in 4 configurations have been reported.

  tpool-0: 		No thread pool (--thread-pool-size=0).
  inline-or-pool-3:	Thread pool threshold 3. That is if queue depth is 3
			or less, do online processing otherwise use thread pool.
			Once threshold is crossed, all requests are processed
			by thread pool and no inline processing.
  pool-thresh-3:	Thread pool threshold 3. That is if queue depth is 3
			or less, do inline processing. If number of requests
			received are more than 3, send them to thread pool.
			So if 7 requests are received, first 3 are processed
			inline and rest 4 are sent to thread pool.
  tpool-16: 		Thread pool size is 16 (--thread-pool-size=16)

- Two fio ioengines psync and libaio have been used.

- I/O Workload of randread, radwrite, randrw (mixed), seqread and seqwrite
  have been run.

- Each file size is 4G. Block size 4K. iodepth=16

- "multi" means same operation was done with 4 jobs and each job is
  operating on a file of size 4G.

Conclusion
----------
- "--thread-pool-threshold=3" seems to work reasonably well for low
  as well as high queue depths.
