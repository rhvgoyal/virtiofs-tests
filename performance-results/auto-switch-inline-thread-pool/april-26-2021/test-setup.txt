Purpose
=======
Run tests with and without auto switch for inline and thread-pool processing
of virtiofs requests and measure if auto switch works reasonably well to
immitate high performance both at low queue depths as well as high queue
depths.

Methodology
===========
I have basically run bunch of fio jobs to get a sense of speed of
various operations. I wrote a simple wrapper script to run fio jobs
3 times and take their average and report it. These scripts and fio
jobs are available here.

https://github.com/rhvgoyal/virtiofs-tests

I have a fast optane SSD on host where I setup a test directory and exported
that directory inside guest using virtiofs and ran tests inside guests.

I have used caching mode cache=auto for vitiofs. Ran virtiofsd with option
"-o allow_direct_io" so that host cache is bypassed. Ran all tests with
direct I/O so that guest cache is bypassed too. Only ran tests with 
ioengine psync and libaio as these work with direct I/O. I believe ioengine
mmap can't do direct I/O .

Used numactl to bind both virtiofs and qemu to node 0. This helps with
reducing numa affects and reduce run to run performance variation.

Test Setup
-----------
- A fedora 29 host with 376Gi RAM, 2 sockets (20 cores per socket, 2
  threads per core)

- Using optane SSD on host as backing store. 4 fio files of 4G each.

- Created a VM with 32 VCPUS and 64GB memory.

- Host kernel: 5.12.0-rc1
- Guest kernel: 5.12.0-rc8

- QEMU emulator version 5.2.94 (v6.0.0-rc4-dirty)

virtiofsd command line
----------------------
- Used following options for virtiofsd.

./build/tools/virtiofsd/virtiofsd --socket-path=/tmp/vhostqemu -o source=/mnt/ssd/virtiofs-source/ -o cache=auto -o xattr --thread-pool-size=16 -o allow_direct_io --daemonize

virtiofs qemu configuration
---------------------------
-object memory-backend-file,id=mem,size=64G,mem-path=/dev/shm,share=on -numa node,memdev=mem"

-chardev socket,id=char0,path=/tmp/vhostqemu -device vhost-user-fs-pci,chardev=char0,tag=myfs,queue-size=1024

Test Results
------------
- Results in 4 configurations have been reported.

  no-tpool: 		No thread pool (--thread-pool-size=0).
  tpool-16: 		Thread pool size is 16 (--thread-pool-size=16)
  auto-switch-3:	Auto switch threshold 3. That is if queue depth is 3
			or less, do online processing otherwise use thread pool.
  auto-switch-4:	Auto switch threshold 4. That is if queue depth is 4
			or less, do online processing otherwise use thread pool.

- Two fio ioengines psync and libaio have been used.

- I/O Workload of randread, radwrite, randrw (mixed), seqread and seqwrite
  have been run.

- Each file size is 4G. Block size 4K. iodepth=16

- "multi" means same operation was done with 4 jobs and each job is
  operating on a file of size 4G.

Conclusion
----------
- Auto switch seems to help. It seems to be matching performane of psync
  ioengine which probably does io depth of 1. And it also matches performance
  of libaio engine with does io depth of 16.

- Auto switch threshold of 3 seems to perform little better than auto switch
  threshold of 4 on my machine. It probably will vary setup to setup.
