Purpose
=======
Run tests with sshfs and virtiofs and see how do these perform.

Methodology
===========
I have basically run bunch of fio jobs to get a sense of speed of
various operations. I wrote a simple wrapper script to run fio jobs
3 times and take their average and report it. These scripts and fio
jobs are available here.

https://github.com/rhvgoyal/virtiofs-tests

I have a fast optane SSD on host where I setup a test directory and exported
that directory inside guest using virtiofs and sshfs ran tests inside guests.

I have used caching mode cache=auto for vitiofs. Did not run it with
"-o allow_direct_io" to bypass host page cache because I did not know
if I could bypass host page cache for sshfs. So for these tests, page
cache on host is being used.

Ran all tests with direct I/O so that guest cache is bypassed. But there
are some ioengines like mmap which will use page cache anyway. So read
results accordingly.

Used numactl to bind both virtiofs and qemu to node 0. This helps with
reducing numa affects and reduce run to run performance variation.

Host and Guest caching
======================
- Ran tests with --direct=1. So guest cache should be bypassed for
  ioengine psync and libaio. But for ioengine mmap, I think page cache
  still must have been used.

- Host caching has not been bypassed. So host page cache is in use.

Test Setup
-----------
- A fedora 34 host with 376Gi RAM, 2 sockets (20 cores per socket, 2
  threads per core)

- Using optane SSD on host as backing store. 4 fio files of 4G each.

- Created a VM with 32 VCPUS and 64GB memory.

- Host kernel: 5.13.19-200.fc34.x86_64
- Guest kernel: 5.17.0-rc1

- QEMU emulator version 6.2.50 (v6.2.0-1131-gb367db4812)

virtiofsd command line
----------------------
- Used following options for virtiofsd.

qemu/build/tools/virtiofsd/virtiofsd --socket-path=/tmp/vhostqemu -o source=/mnt/ssd/virtiofs-source/ -o cache=auto -o xattr -o announce_submounts -o sandbox=chroot -o no_killpriv_v2

virtiofs qemu configuration
---------------------------
-object memory-backend-file,id=mem,size=64G,mem-path=/dev/shm,share=on -numa node,memdev=mem"

-chardev socket,id=char0,path=/tmp/vhostqemu -device vhost-user-fs-pci,chardev=char0,tag=myfs,queue-size=1024

sshfs configuration
-------------------
Used following command line to mount sshfs. So used all default options. There
might be knobs which speed up sshfs by caching more things. I am not aware
of these knobs yet. So going with default configuration for now.

sshfs root@<host>:/mnt/ssd/shared-dir/ /mnt/sshfs/

Test Results
------------
- Results in 2 configurations have been reported.

  sshfs:	Results with sshfs
  virtiofs:	Results with virtiofs

- Three fio ioengines psync, libaio and mmap.

- I/O Workload of randread, radwrite, randrw (mixed), seqread and seqwrite
  have been run.

- Each file size is 4G. Block size 4K. iodepth=16

- "multi" means same operation was done with 4 jobs and each job is
  operating on a file of size 4G.

Conclusion
----------
- virtiofs seems much faster as comapred to virtiofs as far as default
  configuration of sshfs is considered. There might be knobs in sshfs which
  make it faster by caching more data/metadata. That has yet to be explored.
